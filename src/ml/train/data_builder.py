import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from src.data.connectors.bybit_connector import BybitConnector
from src.config.settings import SUPPORTED_PAIRS, SUPPORTED_TIMEFRAMES, CANDLE_LIMIT

logger = logging.getLogger(__name__)

class DataBuilder:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.connector = BybitConnector()
    
    def download_training_data(
        self,
        pairs: List[str] = [],
        timeframes: List[str] = [],
        days: int = 365
    ) -> Dict[str, pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            pairs: –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ)
            timeframes: –°–ø–∏—Å–æ–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ)
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        """
        if not pairs:
            pairs = SUPPORTED_PAIRS
        if not timeframes:
            timeframes = SUPPORTED_TIMEFRAMES
        
        data_dict = {}
        
        for pair in pairs:
            for timeframe in timeframes:
                try:
                    logger.info(f"Downloading data for {pair} {timeframe}...")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
                    df = self.connector.get_historical_data(pair, timeframe, days=days)
                    
                    if not df.empty:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á
                        key = f"{pair.replace('/', '')}_{timeframe}"
                        data_dict[key] = df
                        
                        logger.info(f"Downloaded {len(df)} candles for {pair} {timeframe}")
                    else:
                        logger.warning(f"No data received for {pair} {timeframe}")
                        
                except Exception as e:
                    logger.error(f"Error downloading data for {pair} {timeframe}: {e}")
                    continue
        
        return data_dict
    
    def create_labels(
        self,
        df: pd.DataFrame,
        future_periods: int = 5,
        tp_threshold: float = 0.02,
        sl_threshold: float = 0.01
    ) -> pd.Series:
        """
        –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        
        Args:
            df: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
            future_periods: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –±—É–¥—É—â–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            tp_threshold: –ü–æ—Ä–æ–≥ –¥–ª—è Take Profit (2%)
            sl_threshold: –ü–æ—Ä–æ–≥ –¥–ª—è Stop Loss (1%)
            
        Returns:
            Series —Å –º–µ—Ç–∫–∞–º–∏ ('long', 'short', 'none')
        """
        if df.empty or len(df) < future_periods:
            return pd.Series(dtype='object')
        
        labels = []
        
        for i in range(len(df) - future_periods):
            current_price = df['close'].iloc[i]
            future_prices = df['close'].iloc[i+1:i+future_periods+1]
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –∏ –ø–∞–¥–µ–Ω–∏–µ
            max_gain = (future_prices.max() - current_price) / current_price
            max_loss = (current_price - future_prices.min()) / current_price
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–≥–Ω–∞–ª
            if max_gain >= tp_threshold and max_gain > max_loss:
                labels.append('long')
            elif max_loss >= sl_threshold and max_loss > max_gain:
                labels.append('short')
            else:
                labels.append('none')
        
        # –î–æ–±–∞–≤–ª—è–µ–º NaN –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        labels.extend([np.nan] * future_periods)
        
        return pd.Series(labels, index=df.index)
    
    def prepare_training_dataset(
        self,
        data_dict: Dict[str, pd.DataFrame],
        future_periods: int = 5
    ) -> Dict[str, Tuple[np.ndarray, pd.Series]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            data_dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            future_periods: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –±—É–¥—É—â–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (X, y) –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        """
        from src.features.feature_processor import FeatureProcessor
        
        feature_processor = FeatureProcessor()
        training_datasets = {}
        
        for key, df in data_dict.items():
            try:
                logger.info(f"Preparing dataset for {key}...")
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
                labels = self.create_labels(df, future_periods)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                df_with_features = feature_processor.process_features(df)
                
                if df_with_features.empty:
                    logger.warning(f"No features generated for {key}")
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                X = feature_processor.get_feature_matrix(df_with_features)
                
                if X.size == 0:
                    logger.warning(f"No features available for {key}")
                    continue
                
                # --- –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –º–µ—Ç–æ–∫ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
                labels = labels.loc[df_with_features.index]
                valid_indices = ~labels.isna()
                X = X[valid_indices]
                y = labels[valid_indices]
                
                if len(X) > 0 and len(y) > 0:
                    training_datasets[key] = (X, y)
                    logger.info(f"Prepared dataset for {key}: {len(X)} samples")
                else:
                    logger.warning(f"No valid samples for {key}")
                    
            except Exception as e:
                logger.error(f"Error preparing dataset for {key}: {e}")
                continue
        
        return training_datasets
    
    def save_training_data(
        self,
        data_dict: Dict[str, pd.DataFrame],
        output_dir: str = "data/training"
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ CSV —Ñ–∞–π–ª—ã
        
        Args:
            data_dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        for key, df in data_dict.items():
            try:
                filename = f"{key}.csv"
                filepath = os.path.join(output_dir, filename)
                df.to_csv(filepath, index=False)
                logger.info(f"Saved {filename}")
                
            except Exception as e:
                logger.error(f"Error saving {key}: {e}")
    
    def load_training_data(
        self,
        input_dir: str = "data/training"
    ) -> Dict[str, pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–æ–≤
        
        Args:
            input_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        import os
        data_dict = {}
        
        if not os.path.exists(input_dir):
            logger.warning(f"Directory {input_dir} does not exist")
            return data_dict
        
        for filename in os.listdir(input_dir):
            if filename.endswith('.csv'):
                try:
                    key = filename.replace('.csv', '')
                    filepath = os.path.join(input_dir, filename)
                    df = pd.read_csv(filepath)
                    data_dict[key] = df
                    logger.info(f"Loaded {filename}")
                    
                except Exception as e:
                    logger.error(f"Error loading {filename}: {e}")
        
        return data_dict
    
    def get_data_summary(self, data_dict: Dict[str, pd.DataFrame]) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        
        Args:
            data_dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å–æ —Å–≤–æ–¥–∫–æ–π
        """
        if not data_dict:
            return "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        
        summary = "üìä **–°–≤–æ–¥–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º:**\n\n"
        
        for key, df in data_dict.items():
            if not df.empty:
                import pandas as pd
                start_date = pd.to_datetime(df['timestamp'].iloc[0])
                end_date = pd.to_datetime(df['timestamp'].iloc[-1])
                count = len(df)
                summary += f"**{key}:**\n"
                summary += f"‚Ä¢ –ü–µ—Ä–∏–æ–¥: {start_date} - {end_date}\n"
                summary += f"‚Ä¢ –°–≤–µ—á–µ–π: {count:,}\n"
                summary += f"‚Ä¢ –î–Ω–µ–π: {(end_date - start_date).days}\n\n"
        
        return summary
