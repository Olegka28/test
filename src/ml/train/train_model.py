import pandas as pd
import numpy as np
import json
import os
from typing import Dict, List, Tuple, Optional
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
import logging
from src.features.feature_processor import FeatureProcessor
from src.config.settings import MODEL_DIR, MODEL_PREFIX, FEATURE_WINDOW
from sklearn.preprocessing import LabelEncoder

logger = logging.getLogger(__name__)

# --- –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ numpy —Ç–∏–ø–æ–≤ ---
def to_serializable(val):
    if isinstance(val, np.generic):
        return val.item()
    if isinstance(val, np.ndarray):
        return val.tolist()
    if isinstance(val, dict):
        return {k: to_serializable(v) for k, v in val.items()}
    if isinstance(val, list):
        return [to_serializable(v) for v in val]
    return val

class ModelTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, model_dir: str = MODEL_DIR):
        self.model_dir = model_dir
        self.feature_processor = FeatureProcessor()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(model_dir, exist_ok=True)
    
    def prepare_dataset(self, df: pd.DataFrame, window_size: int = FEATURE_WINDOW) -> Tuple[np.ndarray, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            df: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            Tuple: (–ø—Ä–∏–∑–Ω–∞–∫–∏, –º–µ—Ç–∫–∏)
        """
        if df.empty:
            return np.empty((0, 0)), pd.Series(dtype='object')
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_with_features = self.feature_processor.process_features(df)
        
        if df_with_features.empty:
            return np.empty((0, 0)), pd.Series(dtype='object')
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = self.feature_processor.get_feature_matrix(df_with_features)
        
        if X.size == 0:
            return np.empty((0, 0)), pd.Series(dtype='object')
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ (–∑–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫)
        y = self._create_labels(df_with_features)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –Ω–µ—Ç –º–µ—Ç–æ–∫
        valid_indices = ~y.isna()
        X = X[valid_indices]
        y = y[valid_indices]
        
        return X, y
    
    def _create_labels(self, df: pd.DataFrame) -> pd.Series:
        """
        –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–∑–∞–≥–ª—É—à–∫–∞ - –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É)
        
        Args:
            df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            Series —Å –º–µ—Ç–∫–∞–º–∏
        """
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞ - —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏
        np.random.seed(42)
        labels = np.random.choice(['long', 'short', 'none'], size=len(df), p=[0.3, 0.3, 0.4])
        
        return pd.Series(labels, index=df.index, dtype='object')
    
    def train_model(
        self,
        symbol: str,
        timeframe: str,
        X: np.ndarray,
        y: pd.Series,
        test_size: float = 0.2,
        random_state: int = 42
    ) -> Dict[str, any]:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞—Ä—ã –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
            X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            y: –í–µ–∫—Ç–æ—Ä –º–µ—Ç–æ–∫
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            # --- –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–∞ ---
            le = LabelEncoder()
            y_encoded = le.fit_transform(y)

            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded
            )
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_train),
                y=y_train
            )
            weight_dict = dict(zip(np.unique(y_train), class_weights))
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
            model = XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=random_state,
                scale_pos_weight=1.0,
                class_weight=weight_dict
            )
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            logger.info(f"Training model for {symbol} {timeframe}...")
            model.fit(X_train, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
            
            # --- –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è –º–µ—Ç—Ä–∏–∫ ---
            y_test_decoded = le.inverse_transform(y_test)
            y_pred_decoded = le.inverse_transform(y_pred)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            report = classification_report(y_test_decoded, y_pred_decoded, output_dict=True)
            conf_matrix = confusion_matrix(y_test_decoded, y_pred_decoded)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            model_name = f"{MODEL_PREFIX}_{symbol.replace('/', '')}_{timeframe}"
            model_path = os.path.join(self.model_dir, f"{model_name}.json")
            model.save_model(model_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_info = {
                'symbol': symbol,
                'timeframe': timeframe,
                'feature_count': X.shape[1],
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'classes': list(le.classes_),
                'feature_importance': list(model.feature_importances_),
                'feature_names': self.feature_processor.get_feature_names(),
                'metrics': {
                    'accuracy': report['accuracy'] if 'accuracy' in report else None,
                    'precision': report['weighted avg']['precision'] if 'weighted avg' in report else None,
                    'recall': report['weighted avg']['recall'] if 'weighted avg' in report else None,
                    'f1_score': report['weighted avg']['f1-score'] if 'weighted avg' in report else None
                },
                'class_metrics': report
            }
            
            info_path = os.path.join(self.model_dir, f"{model_name}_info.json")
            with open(info_path, 'w') as f:
                json.dump(to_serializable(model_info), f, indent=2)
            
            logger.info(f"Model saved: {model_path}")
            logger.info(f"Accuracy: {report['accuracy']:.4f}")
            
            return {
                'model': model,
                'model_path': model_path,
                'info': model_info,
                'X_test': X_test,
                'y_test': y_test_decoded,
                'y_pred': y_pred_decoded,
                'y_pred_proba': y_pred_proba
            }
            
        except Exception as e:
            logger.error(f"Error training model for {symbol} {timeframe}: {e}")
            raise
    
    def train_all_models(self, data_dict: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, any]]:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        
        Args:
            data_dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            –§–æ—Ä–º–∞—Ç: {'SOLUSDT_15m': df, 'SOLUSDT_1h': df, ...}
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        results = {}
        
        for key, df in data_dict.items():
            try:
                # –ü–∞—Ä—Å–∏–º –∫–ª—é—á
                parts = key.split('_')
                if len(parts) >= 2:
                    symbol = '_'.join(parts[:-1])  # –í—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏
                    timeframe = parts[-1]
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                    X, y = self.prepare_dataset(df)
                    
                    if X.size > 0 and len(y) > 0:
                        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                        result = self.train_model(symbol, timeframe, X, y)
                        results[key] = result
                    else:
                        logger.warning(f"No valid data for {key}")
                        
            except Exception as e:
                logger.error(f"Error training model for {key}: {e}")
                continue
        
        return results
    
    def evaluate_model(self, model_result: Dict[str, any]) -> str:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á–µ—Ç
        
        Args:
            model_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –æ—Ç—á–µ—Ç–æ–º
        """
        info = model_result['info']
        metrics = info['metrics']
        
        report = f"üìä **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {info['symbol']} {info['timeframe']}**\n\n"
        report += f"üìà **–ú–µ—Ç—Ä–∏–∫–∏:**\n"
        report += f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}\n"
        report += f"‚Ä¢ Precision: {metrics['precision']:.4f}\n"
        report += f"‚Ä¢ Recall: {metrics['recall']:.4f}\n"
        report += f"‚Ä¢ F1-Score: {metrics['f1_score']:.4f}\n\n"
        
        report += f"üìã **–î–∞–Ω–Ω—ã–µ:**\n"
        report += f"‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {info['feature_count']}\n"
        report += f"‚Ä¢ –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {info['train_samples']}\n"
        report += f"‚Ä¢ –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {info['test_samples']}\n"
        report += f"‚Ä¢ –ö–ª–∞—Å—Å–æ–≤: {len(info['classes'])}\n\n"
        
        # –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = list(zip(info['feature_names'], info['feature_importance']))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        report += f"üéØ **–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**\n"
        for i, (feature, importance) in enumerate(feature_importance[:10]):
            report += f"{i+1}. {feature}: {importance:.4f}\n"
        
        return report
